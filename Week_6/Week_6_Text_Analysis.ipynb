{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mrsnellek/MSDS_686_22F8W2/blob/22F8W2/Week_6/Week_6_Text_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVqVcVDo9aUi"
      },
      "source": [
        "# Text Analysis Assingment\n",
        "## Adapted from Deep Learning with Python (2021)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuLZ2hhn9aUs"
      },
      "outputs": [],
      "source": [
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gB-uz88U9aVH"
      },
      "outputs": [],
      "source": [
        "# Import the necessary libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import models, layers, backend\n",
        "from tensorflow.keras.layers import TextVectorization, IntegerLookup, Bidirectional, LSTM\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import numpy as np\n",
        "import string\n",
        "import re\n",
        "np.random.seed(1)\n",
        "import os\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj9owpkxgpf_"
      },
      "source": [
        "Download and extract the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPYx5WYmgCxm"
      },
      "outputs": [],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnWxMtbJgYPd"
      },
      "outputs": [],
      "source": [
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtMA9I8Mg9uF"
      },
      "source": [
        "We will removed the unneeded unsup directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E13zVmYGgi7I"
      },
      "outputs": [],
      "source": [
        "!rm -r aclImdb/train/unsup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPebHwST2lPs"
      },
      "source": [
        "We will create a validation folder with validation images from the training images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Io978fS00Hgc"
      },
      "outputs": [],
      "source": [
        "for category in (\"neg\", \"pos\"):\n",
        "  os.makedirs(\"aclImdb/val/\" + category)\n",
        "  files = os.listdir(\"aclImdb/train/\" + category)\n",
        "  np.random.shuffle(files)\n",
        "  num_val_samples = int(0.2*len(files))\n",
        "  val_files = files[-num_val_samples:]\n",
        "  for fname in val_files:\n",
        "    shutil.move(\"aclImdb/train/\" + category + \"/\" + fname,\n",
        "                \"aclImdb/val/\" + category + \"/\" + fname)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(files)"
      ],
      "metadata": {
        "id": "pa3D7pYs-Xkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMKu1b4UbNPW"
      },
      "source": [
        "Just like with our image data we will use flow from directory so we are not using too much memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TKniW8ChJ25"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "train_flow = keras.utils.text_dataset_from_directory(\"aclImdb/train\", batch_size = batch_size)\n",
        "val_flow = keras.utils.text_dataset_from_directory(\"aclImdb/val\", batch_size = batch_size)\n",
        "test_flow = keras.utils.text_dataset_from_directory(\"aclImdb/test\", batch_size = batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HX3wunPDZB6v"
      },
      "source": [
        "\n",
        "We can inspect the first element of data flowing from the training directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SI2anqf9VPSt"
      },
      "outputs": [],
      "source": [
        "for x, y in train_flow:\n",
        "  print(\"input shape: \", x.shape)\n",
        "  print(\"input type: \", x.dtype)\n",
        "  print(\"targets shape: \", y.shape)\n",
        "  print(\"targets type: \", y.dtype)\n",
        "  print(\"x[0] input text: \", x[0])\n",
        "  print(\"y targets: \", y)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZ8eL-fy6Ypv"
      },
      "outputs": [],
      "source": [
        "def custom_standardization(input_data):\n",
        "    lowercase = tf.strings.lower(input_data)\n",
        "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
        "    return tf.strings.regex_replace(\n",
        "        stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz7Vj0Ep_s23"
      },
      "source": [
        "There is some puncuation and `<br /><br />` values that we should remove.  The below function will remove those features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybnbbP0d91kC"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    standardize=custom_standardization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arPjnCUi-BA3"
      },
      "outputs": [],
      "source": [
        "check_train = train_flow.map(lambda x, y: (custom_standardization(x), y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LN4LpbrS-GPU"
      },
      "outputs": [],
      "source": [
        "for x in check_train:\n",
        "  print(\"x input text: \", x[0])\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJ0Zh2-n3tIR"
      },
      "source": [
        "Now we will work on vectorizing the x text data.  We will define the number of most commen tokens, words or n-grams, we will use to build our model.  Below we will start with 20000 most common words or n-grams.  This will make sure we are not modeling on spelling mistakes or words that only appear once in the data, in otherwords we do not want to model 'noise'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZbf5xprNAem"
      },
      "outputs": [],
      "source": [
        "# This removes the y targets.  \n",
        "text_only_train = train_flow.map(lambda x, y: x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEa1tjFBXNFs"
      },
      "source": [
        "Now that we have removed the 'y' targets from the data and removed puncuation and breaks we can begin to vectorize the data. We will define how many words or n-grams as the number of tokens.  In our case we will choose 20,000 of the most common words or n-grams.  We will choose to normalize the output using TF-IDF normalization methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekMVIxyt5ExL"
      },
      "outputs": [],
      "source": [
        "max_tokens = 20000\n",
        "text_vectorization = TextVectorization(\n",
        "    standardize = custom_standardization,\n",
        "    ngrams = 2,\n",
        "    max_tokens = max_tokens,\n",
        "    output_mode = 'tf_idf'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsq9c4-UX-Ao"
      },
      "outputs": [],
      "source": [
        "# This will create a vector that we will use to index our text inputs with values \n",
        "# between 0 and 20,000.\n",
        "text_vectorization.adapt(text_only_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E91Dh-OpVAdf"
      },
      "outputs": [],
      "source": [
        "# Appy text_vecorization to the x input text data, but not the y targets\n",
        "tfid_train = train_flow.map(lambda x, y: (text_vectorization(x), y))\n",
        "tfid_val = val_flow.map(lambda x, y: (text_vectorization(x), y))\n",
        "tfid_test = test_flow.map(lambda x, y: (text_vectorization(x), y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SN-0viasYmO0"
      },
      "source": [
        "Now we apply the text_vecorization to each of of x text inputs as they flowed from our dirrectories.  Below you can see the ouputs.  The x data is now vectorized and there is normalized TF-IDF value for each word or n-gram found in the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgbYOOYVRmcn"
      },
      "outputs": [],
      "source": [
        "for x, y in tfid_train:\n",
        "  print(\"input shape: \", x.shape)\n",
        "  print(\"input type: \", x.dtype)\n",
        "  print(\"targets shape: \", y.shape)\n",
        "  print(\"targets type: \", y.dtype)\n",
        "  print(\"x input text: \", x[0])\n",
        "  print(\"y target: \", y)\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whU8rSA5adfV"
      },
      "source": [
        "Now we can build our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sh5NGRFFz7kF"
      },
      "outputs": [],
      "source": [
        "def text_model():\n",
        "  backend.clear_session()\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Dense(16, activation = 'relu', input_shape = (max_tokens,)))\n",
        "  model.add(layers.Dropout(0.5))\n",
        "  model.add(layers.Dense(1, activation = 'sigmoid'))\n",
        "  model.compile(optimizer = 'rmsprop',\n",
        "               loss = 'binary_crossentropy',\n",
        "               metrics = ['accuracy'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNMNKk0Yahan"
      },
      "source": [
        "Now we apply our model to the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SODHx9pY6GAj"
      },
      "outputs": [],
      "source": [
        "model = text_model()\n",
        "model.fit(tfid_train, validation_data = tfid_val, \n",
        "          epochs = 10, \n",
        "          callbacks=[EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights = True)])\n",
        "model.evaluate(tfid_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bt5fFsF9bSFk"
      },
      "source": [
        "The above model was a bag of words approach with n-grams.  The bag of word approach does not consider the word order with the exeption of the n-grams.  \n",
        "Now we will add an embedding layer which map word associations into geometric space.  We will also add a LSTM layer for added word order awareness.\n",
        "\n",
        "Will will truncate the emdeded work space to 600, much more reasonable than 20,000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGEd7c2AjlEb"
      },
      "outputs": [],
      "source": [
        "max_length = 600\n",
        "max_tokens = 20000\n",
        "text_embeding = TextVectorization(\n",
        "    standardize = custom_standardization,\n",
        "    max_tokens = max_tokens,\n",
        "    output_mode = 'int',\n",
        "    output_sequence_length = max_length\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hofHH26j9bP"
      },
      "outputs": [],
      "source": [
        "# This will create a vector that we will use to index our text inputs with values \n",
        "# between 0 and 20,000.\n",
        "text_embeding.adapt(text_only_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rf9XSjhWkH7H"
      },
      "outputs": [],
      "source": [
        "embed_train = train_flow.map(lambda x, y: (text_embeding(x), y))\n",
        "embed_val = val_flow.map(lambda x, y: (text_embeding(x), y))\n",
        "embed_test = test_flow.map(lambda x, y: (text_embeding(x), y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWpzlcqHkLu6"
      },
      "outputs": [],
      "source": [
        "for x, y in embed_train:\n",
        "  print(\"input shape: \", x.shape)\n",
        "  print(\"input type: \", x.dtype)\n",
        "  print(\"targets shape: \", y.shape)\n",
        "  print(\"targets type: \", y.dtype)\n",
        "  print(\"x input text: \", x[0])\n",
        "  print(\"y input text: \", y)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgfJicKlvWWy"
      },
      "outputs": [],
      "source": [
        "backend.clear_session()\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = layers.Embedding(max_tokens, 128) (inputs) # Here we will learn the word emdeddings\n",
        "x = layers.Bidirectional(layers.LSTM(32)) (x)\n",
        "x = layers.Dropout(0.5) (x)\n",
        "x = layers.Dense(64, activation=\"relu\") (x)\n",
        "x = layers.Dropout(0.5) (x)\n",
        "outputs = layers.Dense(1, activation = 'sigmoid') (x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer = 'rmsprop',\n",
        "              loss = 'binary_crossentropy',\n",
        "              metrics = ['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAfpjOa-zRlp"
      },
      "outputs": [],
      "source": [
        "model.fit(embed_train, validation_data = embed_val, \n",
        "          epochs = 10, \n",
        "          callbacks=[EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights = True)])\n",
        "model.evaluate(embed_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word embedings and LSTM models can  inprove text models. You can also use pretrained word embedings such as [GloVe](https://nlp.stanford.edu/pubs/glove.pdf) or [word2vec](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/rvecs.pdf)  Using these pretained embedings will decrease run time and likely increase accuracy.\n",
        "\n",
        "Transformer model architecure begain being favored over recuent neural networks, RNN, starting in 2017. Here is a link to the seminal paper that introduced transformer nerual networks: https://arxiv.org/pdf/1706.03762.pdf\n",
        "\n",
        "We will not cover Transformers this week but you are welcome to explore and utalize transformers in your HW.\n"
      ],
      "metadata": {
        "id": "q2WaF_Cu_prL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your turn\n",
        "\n",
        "Your assignment is to download and perfrom a sentiment classification of Corona Virus tweets.  the [data set](https://www.kaggle.com/datasets/datatattle/covid-19-nlp-text-classification) is part of a Kaggle competition.  You will need to download the Kaggle API .json file and upload it to Google Colabs.  I have provided the code you need to download, unzip, and organize the tweets so you can perfrom NLP classification.  "
      ],
      "metadata": {
        "id": "8ARoo-tCDmIR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JK-rs0FWzce0"
      },
      "outputs": [],
      "source": [
        "# Download and innsall Kaggle package\n",
        "! pip install kaggle -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cFlJRS0M_x_"
      },
      "outputs": [],
      "source": [
        "# Make a Kaggle dirrectory\n",
        "! mkdir ~/.kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Upload your kaggle.json API file"
      ],
      "metadata": {
        "id": "jzrSGF-Te1rP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPB_inSlNzWg"
      },
      "outputs": [],
      "source": [
        "# Copy your kaggle.json file to the newly creadted Kaggle folder\n",
        "! cp kaggle.json ~/.kaggle/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGc9DwZ7NqSU"
      },
      "outputs": [],
      "source": [
        "# Change permission of the the ,json file.\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZhgvrmVNGX2"
      },
      "outputs": [],
      "source": [
        "# Download the dataset\n",
        "! kaggle datasets download -d datatattle/covid-19-nlp-text-classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9e3RyFxwNXMs"
      },
      "outputs": [],
      "source": [
        "# Unzip the dataset\n",
        "! unzip covid-19-nlp-text-classification.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksOgLUa2T9F9"
      },
      "outputs": [],
      "source": [
        "# Import pandas because file is .csv\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mv_suVzeUAnA"
      },
      "outputs": [],
      "source": [
        "# Load the .csv datasets.  View the number of tweets in each file. \n",
        "cv_train = pd.read_csv('Corona_NLP_train.csv', encoding='latin-1')\n",
        "cv_test = pd.read_csv('Corona_NLP_test.csv', encoding='latin-1')\n",
        "print(len(cv_train))\n",
        "print(len(cv_test))\n",
        "print(cv_train.head())\n",
        "print(cv_train['Sentiment'].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will simplify the problem by creating 3 classes, positive, neutral, and negative."
      ],
      "metadata": {
        "id": "FxnpYGBAKDFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv_train['Sentiment']=cv_train['Sentiment'].replace('Extremely Positive','Positive')\n",
        "cv_train['Sentiment']=cv_train['Sentiment'].replace('Extremely Negative','Negative')\n",
        "\n",
        "cv_test['Sentiment']=cv_test['Sentiment'].replace('Extremely Positive','Positive')\n",
        "cv_test['Sentiment']=cv_test['Sentiment'].replace('Extremely Negative','Negative')"
      ],
      "metadata": {
        "id": "HijbT3x9KCGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAEfYPYqVvrI"
      },
      "outputs": [],
      "source": [
        "# Create CV train and validataion folder\n",
        "for category in cv_train['Sentiment'].unique():\n",
        "  os.makedirs(\"cv/train/\" + category)\n",
        "  os.makedirs(\"cv/val/\" + category)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSuUua9GzfUh"
      },
      "outputs": [],
      "source": [
        "# Place each tweet into each folder as a .txt file\n",
        "i = 0\n",
        "for category in cv_train['Sentiment'].unique():\n",
        "  for tweet in cv_train['OriginalTweet'].loc[cv_train['Sentiment'] == category]:\n",
        "      with open(\"cv/train/\" + category + \"/\" + str(i) + \".txt\", 'w', encoding='utf-8') as my_tweet:\n",
        "        i +=1\n",
        "        my_tweet.write(tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y94GbiTt14jE"
      },
      "outputs": [],
      "source": [
        "# Split 20% of the training tweets into respective validataion folders.\n",
        "for category in cv_train['Sentiment'].unique(): \n",
        "  files = os.listdir(\"cv/train/\" + category)\n",
        "  np.random.shuffle(files)\n",
        "  num_val_samples = int(0.2*len(files))\n",
        "  val_files = files[-num_val_samples:]\n",
        "  for fname in val_files:\n",
        "    shutil.move(\"cv/train/\" + category + \"/\" + fname,\n",
        "                \"cv/val/\" + category + \"/\" + fname)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvuC8LUJ2V_Y"
      },
      "outputs": [],
      "source": [
        "# Place each test tweet into each folder as a .txt file\n",
        "for category in cv_test['Sentiment'].unique():\n",
        "  os.makedirs(\"cv/test/\" + category)\n",
        "i = 0\n",
        "for category in cv_test['Sentiment'].unique():\n",
        "  for tweet in cv_test['OriginalTweet'].loc[cv_test['Sentiment'] == category]:\n",
        "      with open(\"cv/test/\" + category + \"/\" + str(i) + \".txt\", 'w', encoding='utf-8') as my_tweet:\n",
        "        i +=1\n",
        "        my_tweet.write(tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jhywdRLXMPA"
      },
      "outputs": [],
      "source": [
        "# Sanitiy check that files are in correct folders.\n",
        "batch_size = 32\n",
        "cv_train_flow = keras.utils.text_dataset_from_directory(\"cv/train\", batch_size = batch_size, label_mode = 'categorical')\n",
        "cv_val_flow = keras.utils.text_dataset_from_directory(\"cv/val\", batch_size = batch_size, label_mode = 'categorical')\n",
        "cv_test_flow = keras.utils.text_dataset_from_directory(\"cv/test\", batch_size = batch_size, label_mode = 'categorical')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VPROiRmXMPB"
      },
      "source": [
        "### Continue creating a model to classify the tweets into the 3 different categories.  You can try a pretrained word embeding layer, n-grams, TF-IDF, LSTM, removing stopwords, depth, sequence length, etc. Think about how you will clean the tweets and remove special characters and character artifacts."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMrYNKTTpSgVdx6tMpLcIk+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}