{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8EXUxVP1_KJz"
   },
   "source": [
    "# IMDB Regularization and Dropout Example\n",
    "### This Example was adapted from Deep Learning with Python Chapter 3 and 4 Chollet, F. (2017). Deep Learning with Python (1st ed.). Greenwich, CT, USA: Manning Publications Co."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "id": "i2VEtpfG_KJ3",
    "outputId": "1ca63841-6e75-4b52-f3b8-5e4787ff913a"
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OeSG7I-q_KKD"
   },
   "source": [
    "## We will improve our previous IMDB neural net by adding regularization, dropout, batch normalization, and early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "E2itik6K_KKF",
    "outputId": "1d859cff-7a3c-4d8a-89ab-a8c0eb300b70"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.datasets import imdb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# When we import the data we only select the 10,000 most common words in the reviews.\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FDBAuQ58_KKM"
   },
   "source": [
    "## Previous Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gM-KLVv2_KKO"
   },
   "outputs": [],
   "source": [
    "# Remember, we cannot add a single vector to a neural network. The data needs to be \n",
    "# converted to a tensor. This function will create a tensor that is N by 10000.\n",
    "# N is the number of samples and 10000 is the number of unique words. The sparse tensor\n",
    "# will have all zeroes except for ones where that word is in the review\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    word_matrix = tf.sparse.SparseTensor(\n",
    "        # The non-zero locations in each row correspond to the word indices that are found in the document\n",
    "        indices=[[row_idx, word_idx] for row_idx, word_indices in enumerate(sequences) for word_idx in set(word_indices)],\n",
    "        # Use \"1\" as the value of each non-zero index (indicating the word is used in the document)\n",
    "        values=[1 for row_idx, word_indices in enumerate(sequences) for word_idx in set(word_indices)],\n",
    "        # The overall tensor shape\n",
    "        dense_shape=[len(sequences), dimension]\n",
    "    )\n",
    "    # Optimize by ordering the non-zero indices in ascending row-major order\n",
    "    word_matrix = tf.sparse.reorder(word_matrix)\n",
    "    return word_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nu275BlJ_KKe"
   },
   "outputs": [],
   "source": [
    "# Split the training data into a 75/25 train/validation split\n",
    "train_data, valid_data, train_labels, valid_labels = train_test_split(\n",
    "    train_data, train_labels, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8mgPWgtL_KKV"
   },
   "outputs": [],
   "source": [
    "# Apply the vectorize function to the train_data and test_data\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_valid = vectorize_sequences(valid_data)\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jifkQPkB_KKZ"
   },
   "outputs": [],
   "source": [
    "# Convert the y targets to float32\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_valid = np.asarray(valid_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bpsoMbBl_KKk"
   },
   "outputs": [],
   "source": [
    "# Import the keras libraries\n",
    "from keras import models, layers, regularizers\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R6zXWxz-_KKr"
   },
   "outputs": [],
   "source": [
    "# Build the model architecture.  Input shape must equal the number of vectore or (unique words).\n",
    "# Add one hidden layer with 16 nodes.  Keep the activation function as 'relu'\n",
    "# Since we have a binary classification, the output activation function will be 'sigmoid'\n",
    "# We will keep with the 'adam' optomizer function, loss = 'binary_crossentropy', and metrics = 'accuracy'\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation = 'relu', input_shape = (10000,)))\n",
    "model.add(layers.Dense(16, activation = 'relu'))\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "             loss = 'binary_crossentropy',\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 770
    },
    "colab_type": "code",
    "id": "ynS0_PBj_KKv",
    "outputId": "063911fc-5322-4b5d-951b-a1756e1dced4"
   },
   "outputs": [],
   "source": [
    "# Fit the model to the training data. \n",
    "history = model.fit(x_train,\n",
    "                   y_train,\n",
    "                   epochs = 20,\n",
    "                   batch_size = 500,\n",
    "                   validation_data = (x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 403
    },
    "colab_type": "code",
    "id": "2RFUMid2COsL",
    "outputId": "bcee71c5-7af1-443a-f620-8fa55be27e92"
   },
   "outputs": [],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "display(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kBLXN2Tv_KK1"
   },
   "outputs": [],
   "source": [
    "# Let's plot the Loss vs Epochs and Accuracy vs Epochs\n",
    "import matplotlib.pyplot as plt\n",
    "history_dict = history.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "acc_values = history_dict['accuracy']\n",
    "val_acc_values = history_dict['val_accuracy']\n",
    "epochs = range(1, len(history_dict['accuracy']) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "wIxVYjjD_KK6",
    "outputId": "9380af2d-14b3-4bc2-c588-fc12ec0e6299"
   },
   "outputs": [],
   "source": [
    "plt.plot(epochs, loss_values, 'bo', label = 'Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'b', label = 'Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "9jBksF8Y_KK-",
    "outputId": "4ccc0a05-6930-43ed-cfea-af2df30ba11f"
   },
   "outputs": [],
   "source": [
    "plt.plot(epochs, acc_values, 'bo', label = 'Training accuracy')\n",
    "plt.plot(epochs, val_acc_values, 'b', label = 'Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "colab_type": "code",
    "id": "tF2siPvR_KLC",
    "outputId": "a83cb618-dff6-45e2-cbe7-cdb9e44dba64"
   },
   "outputs": [],
   "source": [
    "# It looks like the Validation Loss increases after about 4 Epochs and the validation accuracy decreases after about \n",
    "# 4 epochs.  Lets refit the data but only use 4 epochs and then apply the model on the test data\n",
    "backend.clear_session()\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation = 'relu', input_shape = (10000,)))\n",
    "model.add(layers.Dense(16, activation = 'relu'))\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "             loss = 'binary_crossentropy',\n",
    "             metrics = ['accuracy'])\n",
    "model.fit(x_train,\n",
    "          y_train,\n",
    "          epochs = 4,\n",
    "          batch_size = 500)\n",
    "results = model.evaluate(x_test, y_test)\n",
    "print(model.metrics_names)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JteALhvE_KLH"
   },
   "source": [
    "## Let us try to improve on our above accuracy by using regularization, dropout, batch normalization, and early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 830
    },
    "colab_type": "code",
    "id": "dwlc5k8U_KLM",
    "outputId": "a44ee2df-78df-4d98-d919-18b47f92c0a2"
   },
   "outputs": [],
   "source": [
    "# Add Dropouts, batch normalization, add more hidden layers with more hidden units\n",
    "backend.clear_session()\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation = 'relu', kernel_regularizer = regularizers.l1_l2(l1 = 0.001, l2 = 0.001), input_shape = (10000,)))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(layers.Dense(16, kernel_regularizer = regularizers.l1_l2(l1 = 0.001, l2 = 0.001), activation = 'relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "             loss = 'binary_crossentropy',\n",
    "             metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(x_train,\n",
    "                   y_train,\n",
    "                   epochs = 20,\n",
    "                   batch_size = 500,\n",
    "                   validation_data = (x_valid, y_valid),\n",
    "                   callbacks=[EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights = True)])\n",
    "\n",
    "history_dict = history.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "acc_values = history_dict['accuracy']\n",
    "val_acc_values = history_dict['val_accuracy']\n",
    "epochs = range(1, len(history_dict['accuracy']) + 1)\n",
    "\n",
    "plt.plot(epochs, loss_values, 'bo', label = 'Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'b', label = 'Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(epochs, acc_values, 'bo', label = 'Training accuracy')\n",
    "plt.plot(epochs, val_acc_values, 'b', label = 'Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "results = model.evaluate(x_test, y_test)\n",
    "print(model.metrics_names)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "IMDB_Regularization_and_Dropout_Example.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
