{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Deep_Learning_'Hello_MNIST'.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "2-fHqdpQ4-hm"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1BMmhrW4-VF"
      },
      "source": [
        "# A tutorial introduction into deep learning with Keras and Tensorflow.  We will use the MNIST dataset which is the 'Hello world' problem of deep learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfjjxqk-4-VY"
      },
      "source": [
        "I always like to start my jupternotebooks with this code because it fits the display window to my screen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oq3zQaj94-Vf"
      },
      "source": [
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTIiJaMH4-We"
      },
      "source": [
        "### This tutrial was adapted from Deep Learning with Python Chollet, F. (2021). Deep Learning with Python (2nd ed.). Greenwich, CT, USA: Manning Publications Co."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7J8caRE4-Wv"
      },
      "source": [
        "Start with some definitions.\n",
        "Numerical data in an array are called tensors.  https://en.wikipedia.org/wiki/Tensor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYhqLSfB4-W5"
      },
      "source": [
        "Scalars are 0 dimensional tensors (a single digit). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sq1dfmTjXx1r"
      },
      "source": [
        "import numpy as np\n",
        "x = np.array(12)\n",
        "print('The value of x is', x)\n",
        "print('The dimension of this tensor is', x.ndim) # 0 dimensions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yd6j127g4-Xg"
      },
      "source": [
        "A 1 dimensional tensor is also called a vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iWxjAcgYF2H"
      },
      "source": [
        "x = np.array([12, 1, 2, 3]) #create a vector\n",
        "print('The value of x is', x)\n",
        "print('The dimention of this tensor is', x.ndim) # 1 dimensions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZnkKqX74-YS"
      },
      "source": [
        "A 2 dimensional tensor is also called a matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kw6vcniRYMga"
      },
      "source": [
        "x = np.array([[12, 1, 2, 3],\n",
        "              [5, 6, 7, 8,],\n",
        "              [10, 11, 12, 12]])\n",
        "print('The value of x is', x) # Print the 3 x 4 matrix\n",
        "print('The dimension of this tensor is', x.ndim) # 2 dimensions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDXBkNgF4-ZH"
      },
      "source": [
        "We can create n dimensional tensors easily, although they become difficult to visualize.\n",
        "This 3D tensor is like a cube of data.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7unoEvdhYbYR"
      },
      "source": [
        "x = np.array([[[12, 1, 2, 3],\n",
        "               [5, 6, 7, 8,],\n",
        "               [10, 11, 12, 12]],\n",
        "              [[2, 2, 2, 2,],\n",
        "               [3,3,3,3],\n",
        "               [4,4,4,4]],\n",
        "              [[5,5,5,5],\n",
        "               [6,6,6,6],\n",
        "               [7,7,7,7]]])\n",
        "print('The value of x is', x)\n",
        "print('The dimension of this tensor is', x.ndim) # 3 dimensional array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QrNkyOM4-Z0"
      },
      "source": [
        "#### Reshaping tensors is important concept to understand.  We can reshape a tensor as long as it has the same number of coefficients as the initial tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5jul5ly4-Z6"
      },
      "source": [
        "x = x.reshape(3*3*4,1)\n",
        "print(x)\n",
        "x = x.reshape(4, 3*3)\n",
        "print(x)\n",
        "x = x.reshape(2, 18)\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihVQabEo4-aS"
      },
      "source": [
        "##### Tensors have three atributes: number of axis (dimensions), shape (length of each axis), and data type (typically we will use float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also manipulate tensors with TensorFlow"
      ],
      "metadata": {
        "id": "s2vP9ckYvLuh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Tensorflow as tf\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "0XE9M6y1vULu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.ones(shape=(2,1))\n",
        "print(x)"
      ],
      "metadata": {
        "id": "diW1WriOvbms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Tensorflow variable\n",
        "v = tf.Variable(initial_value=tf.random.normal(shape=(3,1)))\n",
        "print(v)"
      ],
      "metadata": {
        "id": "ZMv_KG8pv6iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Once the variable is createed it can be modified using assign\n",
        "v2 = v.assign(tf.random.normal(shape=(3,1)))\n",
        "print(v2)"
      ],
      "metadata": {
        "id": "iljsFGcewSIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we can perform some math operations on the tensors\n",
        "np.dot(v, v2)"
      ],
      "metadata": {
        "id": "FGkCIydvw2Ks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We get the above error because the shapes of v and v2 do not align properly for a dot product.\n",
        "print(v.shape)\n",
        "print(v2.shape)\n",
        "\n",
        "# For a dot product to alight the column rows of X must match the rows of Y. (See Figure 2.5 in book)\n",
        "# Therefore, we must transpost v2"
      ],
      "metadata": {
        "id": "xdzH71UFxQ2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v3 = np.transpose(v2)\n",
        "print(v.shape)\n",
        "print(v3.shape)"
      ],
      "metadata": {
        "id": "uSctb3a0x8l-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now the rows of v match the columns of v3 we can take the dot product\n",
        "np.dot(v, v3)"
      ],
      "metadata": {
        "id": "l560ng4tyOdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUA1N4pS4-aY"
      },
      "source": [
        "# Let's build our first neural net\n",
        "\n",
        "Load the MNIST library which is part of Keras.  MNIST stands for Modified National Institute of Technology. https://en.wikipedia.org/wiki/MNIST_database. It is a collection of 60,000 training and 10,000 test images of the digits 0-9. https://keras.io/datasets/. We will build a deep learning nerual net model to classify the 10 digits. This is the 'Hello World' problem of deep learning. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AG-iMB8P3DZ"
      },
      "source": [
        "from tensorflow.keras.datasets import mnist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Efu6_zaQKtQ"
      },
      "source": [
        "(train_images, train_labels),(test_images, test_labels) = mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYu3aOo4Qn9R"
      },
      "source": [
        "train_images.shape #60,000 images that are 28 pixles by 28 pixles."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73Jt87YRZBXe"
      },
      "source": [
        "train_images.ndim #3D tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_fgMe2h4-bm"
      },
      "source": [
        "print('The maximum value in the array is', train_images.max()) # The maximum value in the array is 255\n",
        "print('he minimum value in the array is', train_images.min()) # The minimum value in the array is 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACNHKDMhZMtH"
      },
      "source": [
        "# Get the shape, dimensions, max and min value of the test images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvcS2anBQ8cd"
      },
      "source": [
        "print('test image shape:', test_images.shape)\n",
        "print('number of dimensions:', test_images.ndim)\n",
        "print('maximum value', test_images.max())\n",
        "print('minimum value:', test_images.min())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAWThxtY4-co"
      },
      "source": [
        "In general the first axis in a tensor is the samples, the second axis is height, the third axis is the width, and the fourth is color channels (RGB = 3 & BW = 1)\n",
        "So image data will be a 4D tensor [samples, height, width, channels] the MNIST data is 3D bacause the color channel is black and white and thus = 1\n",
        "Video data will be a 5D tensor [samples, frames, height, width, channels]. By convention, time series data will be placed on the secod axis when present"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kavsqyWG4-cZ"
      },
      "source": [
        "Let's view one of the images.  We need to import matplotlib to view the digits "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwEiakpiZXnf"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvYknn0hZf51"
      },
      "source": [
        "digit = train_images[4] # Select the fouth sample.\n",
        "plt.imshow(digit, cmap=plt.cm.binary) # Show the sample.  cmap is the color map.  We will keep it black and white (binary)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 4th train image looks like the number 9.  Lets make sure the label matches."
      ],
      "metadata": {
        "id": "8_eEPRIC9bb1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srjlYOAavZUW"
      },
      "source": [
        "train_labels[4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSBcnrgsRPJ3"
      },
      "source": [
        "# Import models and layers from the tenforflow and keras libraries\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be working with sequential models and dense layers. More on what those mean later.  Another name for a dense layer is a fully connected layer.  The dense layer must be one dimentional. Therefore, the input shape must be reshaped into a vector. There are 60,000 test images with a shape of 28 x 28. We will reshape the matrix into a vector that is 28 * 28 == 784.\n",
        "\n",
        "We pick the 'relu' activation function for our first layer and our output layer activation function is 'softmax' because we have a multi-classification problem."
      ],
      "metadata": {
        "id": "CWKxgF2L9_xG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8lxhcrHRV0x"
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
        "model.add(layers.Dense(10, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we complie the model.  We use the 'adam' optimizer and choose 'catergorical crossentropy' for the loss function because it is a multi-classification problem. We will evaluate our model accuracy."
      ],
      "metadata": {
        "id": "JkdeEsM8FztX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcixP8_xR5j7"
      },
      "source": [
        "model.compile(optimizer = 'adam',\n",
        "               loss = 'categorical_crossentropy',\n",
        "               metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the model is built and compiled we need to process the images for the model.  The images need to be reshaped into a vector of the same dimentions as the input shape above.  We also normalize the values of the images to be between 0 and 1."
      ],
      "metadata": {
        "id": "ERD4fJuF_n2l"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEeckA5ESIMB"
      },
      "source": [
        "train_images =  train_images.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype('float32')/train_images.max()\n",
        "\n",
        "test_images =  test_images.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype('float32')/test_images.max()\n",
        "\n",
        "print(train_images.ndim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bw_1wSzI4-ex"
      },
      "source": [
        "print('train image shape:', train_images.shape)\n",
        "print('number of dimensions:', train_images.ndim)\n",
        "print('maximum value', train_images.max())\n",
        "print('minimum value:', train_images.min())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can always get our images back by reshaping to a matrix.\n",
        "plt.imshow(train_images.reshape((60000,28,28))[4], cmap=plt.cm.binary)"
      ],
      "metadata": {
        "id": "q8QVj91nADix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeczsZHUVDQx"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to convert the labels into catergorical values.\n",
        "# We can check the train lables for the 4th value to ensure\n",
        "# it is labeled as 9\n",
        "print(train_labels[4])"
      ],
      "metadata": {
        "id": "a9u7dxrQGsi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w90ZrgWhVPC1"
      },
      "source": [
        "# Batch size is how many images to process at once. \n",
        "# Epoch is how many times to repeat the analysis.  \n",
        "# Each epoch performs 500 gradient updates (60,000/120 = 500)\n",
        "model.fit(train_images, train_labels, epochs = 5, batch_size = 120) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHqqd_YYVdz3"
      },
      "source": [
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print('test_acc:', test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIRCQyZ2F24o"
      },
      "source": [
        "# Your Turn.\n",
        "# Below is your assignmnet\n",
        "####  Build 3 different models with activation 'relu'.  The last activation must be 'softmax' since we have a multiclass problem.  You will comple the three different models with different optimizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgHnMI6T4-f6"
      },
      "source": [
        "model1 = models.Sequential()\n",
        "model1.add(layers.Dense(512, activation='',input_shape=(28 * 28,)))\n",
        "model1.add(layers.Dense(10, activation=''))\n",
        "\n",
        "model2 = models.Sequential()\n",
        "model2.add(layers.Dense(512, activation='',input_shape=(28 * 28,)))\n",
        "model2.add(layers.Dense(10, activation=''))\n",
        "\n",
        "\n",
        "model3 = models.Sequential()\n",
        "model3.add(layers.Dense(512, activation='',input_shape=(28 * 28,)))\n",
        "model3.add(layers.Dense(10, activation=''))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnUDSgXk4-gE"
      },
      "source": [
        "#### Compile your three models with three different optimizers. Page 89 - 90 of the tetbook list some different optimizers.  You can also find more optimizers and documentation here: https://keras.io/api/optimizers/\n",
        "#### Use categorical_crossentropy for loss since this problem is a multiclassification problem. Metrics will be 'accuracy'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhHGydeC4-gH"
      },
      "source": [
        "model1.compile(optimizer = '',\n",
        "               loss = '',\n",
        "               metrics = [''])\n",
        "\n",
        "model2.compile(optimizer = '',\n",
        "               loss = '',\n",
        "               metrics = [''])\n",
        "\n",
        "model3.compile(optimizer = '',\n",
        "               loss = '',\n",
        "               metrics = [''])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7uSP2vm4-gR"
      },
      "source": [
        "#### Fit the models with epochs = 5 and  batch_size = 150"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9qFLRMA4-gT"
      },
      "source": [
        "model1.fit(train_images, train_labels, epochs = 5, batch_size = 120)\n",
        "model2.fit(train_images, train_labels, epochs = 5, batch_size = 120)\n",
        "model3.fit(train_images, train_labels, epochs = 5, batch_size = 120)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB4RPCz54-gb"
      },
      "source": [
        "#### Test the accuracy of the model on the test images and test labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7V6DjwV4-gc"
      },
      "source": [
        "test_loss, test_acc = model1.evaluate(test_images, test_labels)\n",
        "print('model1_test_acc:', test_acc)\n",
        "\n",
        "test_loss, test_acc = model2.evaluate(test_images, test_labels)\n",
        "print('model2_test_acc:', test_acc)\n",
        "\n",
        "test_loss, test_acc = model3.evaluate(test_images, test_labels)\n",
        "print('model3_test_acc:', test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbR0ds5G4-gm"
      },
      "source": [
        "# Which optimizer gave the highest accuracy? Write you answer below\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bqn4SN0ka7bt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using the opiomizer that gave the highest accuracy compile 3 different models with 3 hidden layers and varying units in each hidden layer.  The first  layer is given to you."
      ],
      "metadata": {
        "id": "rD5Pjrqpa36e"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTh369rD4-gr"
      },
      "source": [
        "h1_model = models.Sequential()\n",
        "h1_model.add(layers.Dense(512, activation='relu',input_shape=(28 * 28,)))\n",
        "h1_model.add(layers.Dense(, activation=''))\n",
        "h1_model.add(layers.Dense(, activation=''))\n",
        "h1_model.add(layers.Dense(, activation=''))\n",
        "h1_model.add(layers.Dense(, activation=''))\n",
        "\n",
        "h2_model = models.Sequential()\n",
        "h2_model.add(layers.Dense(512, activation='relu',input_shape=(28 * 28,)))\n",
        "h2_model.add(layers.Dense(, activation=''))\n",
        "h2_model.add(layers.Dense(, activation=''))\n",
        "h2_model.add(layers.Dense(, activation=''))\n",
        "h2_model.add(layers.Dense(, activation=''))\n",
        "\n",
        "\n",
        "h3_model = models.Sequential()\n",
        "h3_model.add(layers.Dense(512, activation='',input_shape=(28 * 28,)))\n",
        "h3_model.add(layers.Dense(, activation=''))\n",
        "h3_model.add(layers.Dense(, activation=''))\n",
        "h3_model.add(layers.Dense(, activation=''))\n",
        "h3_model.add(layers.Dense(, activation=''))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY-vLrzS4-g1"
      },
      "source": [
        "#### Complie the three models with the best optimizer from above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvYLN-Ld4-g3"
      },
      "source": [
        "h1_model.compile(optimizer = '',\n",
        "               loss = '',\n",
        "               metrics = [''])\n",
        "\n",
        "h2_model.compile(optimizer = '',\n",
        "               loss = '',\n",
        "               metrics = [''])\n",
        "\n",
        "h3_model.compile(optimizer = '',\n",
        "               loss = '',\n",
        "               metrics = [''])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRMZ56yI4-hF"
      },
      "source": [
        "#### Fit the models with epochs = 5 and  batch_size = 120"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wO_mcJ9x4-hI"
      },
      "source": [
        "h1_model.fit()\n",
        "h2_model.fit()\n",
        "h3_model.fit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0Kcmj6x4-hU"
      },
      "source": [
        "#### Test the accuracy of the 3 models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRUw9R5F4-ha"
      },
      "source": [
        "test_loss, test_acc = h1_model.evaluate(test_images, test_labels)\n",
        "print('h1_model_test_acc:', test_acc)\n",
        "\n",
        "test_loss, test_acc = h2_model.evaluate(test_images, test_labels)\n",
        "print('h2_model_test_acc:', test_acc)\n",
        "\n",
        "test_loss, test_acc = h3_model.evaluate(test_images, test_labels)\n",
        "print('h3_model_model_model_test_acc:', test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-fHqdpQ4-hm"
      },
      "source": [
        "#### Which model gave the highest accuracy? Write you answer below."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "knNmeGISay9n"
      }
    }
  ]
}