{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "IMDB_Example.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHIcU-rhBVTC"
      },
      "source": [
        "### This Example was adapted from Deep Learning with Python Chapter 4 Chollet, F. (2021). Deep Learning with Python (2nd ed.). Greenwich, CT, USA: Manning Publications Co."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJT97cWwBVTL"
      },
      "source": [
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-loGSsjBVTf"
      },
      "source": [
        "### This is an example of a binary classification problem using the prepackaged IMDB dateset from the Keras library.  The IMDB dataset has 50,000 training and test reviews form IMDB.  We will use a deep neural net to classify the rewiews as positive or negative."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVPg7CTlBVTj"
      },
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "np.random.seed(1)\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
        "# When we import the data we only select the 10,000 most common words in the reviews.\n",
        "# We do this because if we trained on the least common words we would be training\n",
        "# on words that may only appear in a handful or less of samples. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUsobmqaBVT0"
      },
      "source": [
        "# The train_labels and test_labels consist of 0 and 1, representing negative and positve reviews, respectivly.\n",
        "print(train_labels[0:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSxzOsGRBVUF"
      },
      "source": [
        "# There are 25,000 traing and text data \n",
        "print(train_data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDQghaRuBVUY"
      },
      "source": [
        "# The train_data and test_data are already converted into word indices.  \n",
        "# Each number below represents a word from the word index.  \n",
        "# The word index is 10,000 digits: 0 to 9999\n",
        "print(train_data[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkw6S4slBVUl"
      },
      "source": [
        "# Let's get and print the word index\n",
        "wi = imdb.get_word_index()\n",
        "print(wi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOop6W1oBVUx"
      },
      "source": [
        "# We can sort the items in the word index and return the 50 most common words\n",
        "import operator\n",
        "sorted(wi.items(), key=operator.itemgetter(1))[:50]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "6osEUV7eBVU8"
      },
      "source": [
        "# We can convert the word index back to a written review:\n",
        "reverse_word_index = dict(\n",
        "    [(value, key) for (key, value) in wi.items()])\n",
        "decoded_review = ' '.join(\n",
        "    [reverse_word_index.get(i-3, '?') for i in train_data[10000]])\n",
        "print(decoded_review)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDTaA-bmBVVI"
      },
      "source": [
        "# We cannot add a single vector to a neural network. The data needs to be converted to a tensor.  This function will create a tenor that is 25000 by 10000. 25000 is the number of samples\n",
        "# and 10000 is the number of uniqe words. The tensor will have all zeros except for ones where that word is in the rewview\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1\n",
        "    return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zaQxk1OBVVV"
      },
      "source": [
        "# Apply the vectorize function to the train_data and test_data\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGwaSGqLBVVk"
      },
      "source": [
        "# View the train shape, values, and dimensions\n",
        "print(x_train.shape)\n",
        "print(x_train[0])\n",
        "print(x_train.ndim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcAtt1RKBVVy"
      },
      "source": [
        "# Convert the y targets to float32\n",
        "y_train = np.asarray(train_labels).astype('float32')\n",
        "y_test = np.asarray(test_labels).astype('float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OirApCApBVV-"
      },
      "source": [
        "# Import the keras libraries\n",
        "from tensorflow.keras import models, layers\n",
        "from tensorflow.keras.layers import Flatten, Dense"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_7s0ngiBVWH"
      },
      "source": [
        "# Build the model architecture.  Input shape must equal the number of vectors or (unique words).\n",
        "# Add one hidden layer with 16 units.  Keep the activation function as 'relu'\n",
        "# Since we have a binary classification, the output activation function will be 'sigmoid'\n",
        "# We will keep with the 'rmsprop' optomizer function, loss = 'binary_crossentropy', and metrics = 'accuracy'\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, activation = 'relu', input_dim = (10000)))\n",
        "model.add(layers.Dense(16, activation = 'relu'))\n",
        "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
        "model.compile(optimizer='rmsprop',\n",
        "             loss = 'binary_crossentropy',\n",
        "             metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgvGO4vYBVWR"
      },
      "source": [
        "# Split the Train data into train and validataion sets.\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, shuffle= True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVZJkYNmBVWY"
      },
      "source": [
        "# Fit the model to the training data. \n",
        "history = model.fit(x_train,\n",
        "                   y_train,\n",
        "                   epochs = 20,\n",
        "                   batch_size = 512,\n",
        "                   validation_data = (x_valid, y_valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxmsI0VjpkEP"
      },
      "source": [
        "import pandas as pd\n",
        "# Use this bit of code to view the History output.\n",
        "hist = pd.DataFrame(history.history)\n",
        "print(hist.tail())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bG4DHP6BVWm"
      },
      "source": [
        "# Let's plot the Loss vs Epochs and Accuracy vs Epochs\n",
        "import matplotlib.pyplot as plt\n",
        "history_dict = history.history\n",
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "acc_values = history_dict['accuracy']\n",
        "val_acc_values = history_dict['val_accuracy']\n",
        "epochs = range(1, len(history_dict['accuracy']) + 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qOovQkeBVWs"
      },
      "source": [
        "plt.plot(epochs, loss_values, 'bo', label = 'Training loss')\n",
        "plt.plot(epochs, val_loss_values, 'b', label = 'Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLWJU3emBVW4"
      },
      "source": [
        "plt.plot(epochs, acc_values, 'bo', label = 'Training accuracy')\n",
        "plt.plot(epochs, val_acc_values, 'b', label = 'Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7T4Iw7bzBVW_"
      },
      "source": [
        "# It looks like the Validation Loss increases after about 3 Epochs and the validation accuracy decreases after about \n",
        "# 5 epochs.  Lets refit the data but only use 3 epochs and then apply the model on the test data\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, activation = 'relu', input_shape = (10000,)))\n",
        "model.add(layers.Dense(16, activation = 'relu'))\n",
        "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "             loss = 'binary_crossentropy',\n",
        "             metrics = ['accuracy'])\n",
        "\n",
        "model.fit(x_train,\n",
        "          y_train,\n",
        "          epochs = 3,\n",
        "          batch_size = 500)\n",
        "results = model.evaluate(x_test, y_test)\n",
        "print(model.metrics_names)\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kf-0hLnVBVXH"
      },
      "source": [
        "# Apply to model to the test data and view the results\n",
        "model.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QksvRLCvm973"
      },
      "source": [
        "# View the predicted results rounded to a 1 or 0\n",
        "ans = model.predict(x_test)\n",
        "np.rint(ans)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LXIHY7ZnGi9"
      },
      "source": [
        "# Let suse our decode_review code from above to see if our predicted \n",
        "# classes make sense.\n",
        "reverse_word_index = dict(\n",
        "    [(value, key) for (key, value) in wi.items()])\n",
        "decoded_review = ' '.join(\n",
        "    [reverse_word_index.get(i-3, '?') for i in test_data[0]])\n",
        "print(decoded_review)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Looks like the first test_data review is negative which matches our prediction.\n",
        "np.rint(ans)[0]"
      ],
      "metadata": {
        "id": "HI5kSI9QqPI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fJVz-F7Lq2Is"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}