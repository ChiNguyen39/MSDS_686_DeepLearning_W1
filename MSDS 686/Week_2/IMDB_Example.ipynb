{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JHIcU-rhBVTC"
      },
      "source": [
        "# IMDB Example\n",
        "This Example was adapted from Deep Learning with Python Chapter 4 Chollet, F. (2021). Deep Learning with Python (2nd ed.). Greenwich, CT, USA: Manning Publications Co."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJT97cWwBVTL"
      },
      "outputs": [],
      "source": [
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "m-loGSsjBVTf"
      },
      "source": [
        "### This is an example of a binary classification problem using the prepackaged IMDB dateset from the Keras library.  The IMDB dataset has 50,000 training and test reviews form IMDB.  We will use a deep neural net to classify the reviews as positive or negative."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVPg7CTlBVTj"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import imdb\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "\n",
        "np.random.seed(1)\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
        "# When we import the data we only select the 10,000 most common words in the reviews.\n",
        "# We do this because if we trained on the least common words we would be training\n",
        "# on words that may only appear in a handful or less of samples. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUsobmqaBVT0"
      },
      "outputs": [],
      "source": [
        "# The train_labels and test_labels consist of 0 and 1, representing negative and positve reviews, respectively.\n",
        "print(train_labels[0:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSxzOsGRBVUF"
      },
      "outputs": [],
      "source": [
        "# There are 25,000 training examples\n",
        "print(train_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDQghaRuBVUY"
      },
      "outputs": [],
      "source": [
        "# The train_data and test_data are already converted into word indices.  \n",
        "# Each number below represents a word from the word index.  \n",
        "# The word index is 10,000 digits: 0 to 9999\n",
        "print(train_data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkw6S4slBVUl"
      },
      "outputs": [],
      "source": [
        "# Let's get and print the word index\n",
        "wi = imdb.get_word_index()\n",
        "print(wi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOop6W1oBVUx"
      },
      "outputs": [],
      "source": [
        "# We can sort the items in the word index and return the 50 most common words\n",
        "import operator\n",
        "sorted(wi.items(), key=operator.itemgetter(1))[:50]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6osEUV7eBVU8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# We can convert the word index back to a written review:\n",
        "reverse_word_index = dict(\n",
        "    [(value, key) for (key, value) in wi.items()])\n",
        "decoded_review = ' '.join(\n",
        "    [reverse_word_index.get(i-3, '?') for i in train_data[10000]])\n",
        "print(decoded_review)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split our training examples into training/validation subsets\n",
        "train_data, valid_data, train_labels, valid_labels = train_test_split(\n",
        "    train_data, train_labels, test_size=0.2, shuffle= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zaQxk1OBVVV"
      },
      "outputs": [],
      "source": [
        "# We cannot add a single vector to a neural network. The data needs to be converted to a tensor.\n",
        "# This will create sparse tensors that are 25000-by-10000. 25000 is the number of samples\n",
        "# and 10000 is the number of unique words. The tensor will have all zeros except for ones in \n",
        "# indices where a given word is in the review. Rather than storing a bunch of zero values,\n",
        "# we will only store the locations and values of the non-zero entries of the tensor -- much\n",
        "# more memory efficient!\n",
        "\n",
        "x_train = tf.sparse.SparseTensor(\n",
        "    indices=[[row_idx, word_idx] for row_idx, word_indices in enumerate(train_data) for word_idx in set(word_indices)],\n",
        "    values=[1 for row_idx, word_indices in enumerate(train_data) for word_idx in set(word_indices)],\n",
        "    dense_shape=[train_data.shape[0], 10000]\n",
        "    )\n",
        "x_train = tf.sparse.reorder(x_train)\n",
        "x_valid = tf.sparse.SparseTensor(\n",
        "    indices=[[row_idx, word_idx] for row_idx, word_indices in enumerate(valid_data) for word_idx in set(word_indices)],\n",
        "    values=[1 for row_idx, word_indices in enumerate(valid_data) for word_idx in set(word_indices)],\n",
        "    dense_shape=[valid_data.shape[0], 10000]\n",
        "    )\n",
        "x_valid = tf.sparse.reorder(x_valid)\n",
        "x_test = tf.sparse.SparseTensor(\n",
        "    indices=[[row_idx, word_idx] for row_idx, word_indices in enumerate(test_data) for word_idx in set(word_indices)],\n",
        "    values=[1 for row_idx, word_indices in enumerate(test_data) for word_idx in set(word_indices)],\n",
        "    dense_shape=[test_data.shape[0], 10000]\n",
        "    )\n",
        "x_test = tf.sparse.reorder(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGwaSGqLBVVk"
      },
      "outputs": [],
      "source": [
        "# View the train shape and dimensions\n",
        "print(x_train.shape)\n",
        "print(tf.rank(x_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcAtt1RKBVVy"
      },
      "outputs": [],
      "source": [
        "# Convert the y targets to float32\n",
        "y_train = np.asarray(train_labels).astype('float32')\n",
        "y_valid = np.asarray(valid_labels).astype('float32')\n",
        "y_test = np.asarray(test_labels).astype('float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OirApCApBVV-"
      },
      "outputs": [],
      "source": [
        "# Import the keras libraries\n",
        "from keras import models, layers\n",
        "from keras.layers import Flatten, Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_7s0ngiBVWH"
      },
      "outputs": [],
      "source": [
        "# Build the model architecture.  Input shape must equal the number of vectors or (unique words).\n",
        "# Add one hidden layer with 16 units.  Keep the activation function as 'relu'\n",
        "# Since we have a binary classification, the output activation function will be 'sigmoid'\n",
        "# We will keep with the 'rmsprop' optomizer function, loss = 'binary_crossentropy', and metrics = 'accuracy'\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, activation = 'relu', input_dim = (10000)))\n",
        "model.add(layers.Dense(16, activation = 'relu'))\n",
        "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
        "model.compile(optimizer='rmsprop',\n",
        "             loss = 'binary_crossentropy',\n",
        "             metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVZJkYNmBVWY"
      },
      "outputs": [],
      "source": [
        "# Fit the model to the training data. \n",
        "history = model.fit(x_train,\n",
        "                   y_train,\n",
        "                   epochs = 20,\n",
        "                   batch_size = 512,\n",
        "                   validation_data=(x_valid, y_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxmsI0VjpkEP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# Use this bit of code to view the History output.\n",
        "hist = pd.DataFrame(history.history)\n",
        "print(hist.tail())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bG4DHP6BVWm"
      },
      "outputs": [],
      "source": [
        "# Let's plot the Loss vs Epochs and Accuracy vs Epochs\n",
        "import matplotlib.pyplot as plt\n",
        "history_dict = history.history\n",
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "acc_values = history_dict['accuracy']\n",
        "val_acc_values = history_dict['val_accuracy']\n",
        "epochs = range(1, len(history_dict['accuracy']) + 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qOovQkeBVWs"
      },
      "outputs": [],
      "source": [
        "plt.plot(epochs, loss_values, 'bo', label = 'Training loss')\n",
        "plt.plot(epochs, val_loss_values, 'b', label = 'Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLWJU3emBVW4"
      },
      "outputs": [],
      "source": [
        "plt.plot(epochs, acc_values, 'bo', label = 'Training accuracy')\n",
        "plt.plot(epochs, val_acc_values, 'b', label = 'Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7T4Iw7bzBVW_"
      },
      "outputs": [],
      "source": [
        "# It looks like the Validation Loss increases after about 3 Epochs and the validation accuracy decreases after about \n",
        "# 5 epochs.  Lets refit the data but only use 3 epochs and then apply the model on the test data\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, activation = 'relu', input_shape = (10000,)))\n",
        "model.add(layers.Dense(16, activation = 'relu'))\n",
        "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "             loss = 'binary_crossentropy',\n",
        "             metrics = ['accuracy'])\n",
        "\n",
        "model.fit(x_train,\n",
        "          y_train,\n",
        "          epochs = 3,\n",
        "          batch_size = 512)\n",
        "results = model.evaluate(x_test, y_test)\n",
        "print(model.metrics_names)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kf-0hLnVBVXH"
      },
      "outputs": [],
      "source": [
        "# Apply to model to the test data and view the results\n",
        "model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QksvRLCvm973"
      },
      "outputs": [],
      "source": [
        "# View the predicted results rounded to a 1 or 0\n",
        "ans = model.predict(x_test)\n",
        "np.rint(ans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LXIHY7ZnGi9"
      },
      "outputs": [],
      "source": [
        "# Let's use our decode_review code from above to see if our predicted \n",
        "# classes make sense.\n",
        "reverse_word_index = dict(\n",
        "    [(value, key) for (key, value) in wi.items()])\n",
        "decoded_review = ' '.join(\n",
        "    [reverse_word_index.get(i-3, '?') for i in test_data[0]])\n",
        "print(decoded_review)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HI5kSI9QqPI7"
      },
      "outputs": [],
      "source": [
        "# Looks like the first test_data review is negative which matches our prediction.\n",
        "np.rint(ans)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJVz-F7Lq2Is"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "IMDB_Example.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "60e4dabdb19a4ca7ac63a442dc673a2a524f35d66cbafba0e81abfc8651e6d2a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
