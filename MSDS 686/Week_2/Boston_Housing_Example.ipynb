{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EIQy8YMFnkXR"
      },
      "source": [
        "# Boston Housing Example\n",
        "This Example was adapted from Deep Learning with Python Chapter 4 Chollet, F. (2021). Deep Learning with Python (2nd ed.). Greenwich, CT, USA: Manning Publications Co."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_8G7ob6nkXW"
      },
      "outputs": [],
      "source": [
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "T1aSfEnAnkXi"
      },
      "source": [
        "### This is an example of a neural net regression analysis problem using the prepackaged Boston Housing dataset from the Keras library.  The Boston Housing dataset has only 506 samples. We will use a deep neural net to predict Boston housing prices from 13 features.  You can read more about the dataset here: https://www.kaggle.com/c/boston-housing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FT3JmBMaE04T"
      },
      "outputs": [],
      "source": [
        "# Set the seed for reproducibility\n",
        "import numpy as np\n",
        "np.random.seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RctHBZqrnkXk"
      },
      "outputs": [],
      "source": [
        "# Import the boston housing data set\n",
        "from keras.datasets import boston_housing\n",
        "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NnkCFDMnkXq"
      },
      "outputs": [],
      "source": [
        "# A total of 506 samples split between train and test data.  Each have 13 features used in predicting Boston House prices\n",
        "print(train_data.shape)\n",
        "print(test_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4sJbebfnkXw"
      },
      "outputs": [],
      "source": [
        "# The targets are Boston housing prices in thousands of dollars\n",
        "print(train_targets[1:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlHIZBWmnkX3"
      },
      "outputs": [],
      "source": [
        "# The features have very different ranges and values\n",
        "print(train_data[0:5,:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_UEFTXOnkX8"
      },
      "outputs": [],
      "source": [
        "# Since all the features have very different ranges and values it is best to normalize the data.\n",
        "# We will subtract the mean value from each feature and divide by one standard deviation.\n",
        "# This will center the data in each feature column around zero with unit standard deviation.\n",
        "# Note: we *don't* compute a separate mean/std. deviation for the test data -- transformations\n",
        "# you apply to your test data should only leverage information learned from the training data,\n",
        "# otherwise you are inadvertently \"peeking\" at your test data, which can give you a false\n",
        "# impression of how well your model generalizes.\n",
        "mean = train_data.mean(axis = 0)\n",
        "train_data -= mean\n",
        "\n",
        "std = train_data.std(axis = 0)\n",
        "train_data /= std\n",
        "\n",
        "test_data -= mean\n",
        "test_data /= std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtUTZXI4nkYB"
      },
      "outputs": [],
      "source": [
        "# Now the features have very similar ranges and values\n",
        "print(train_data[0:5,:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Here's another way to accomplish the same task, using\n",
        "# an off-the-shelf preprocessor provided by the\n",
        "# scikit-learn library\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "train_data = scaler.fit_transform(train_data)\n",
        "test_data = scaler.transform(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0ilNSbFnkYH"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import backend\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44R9kzwFnkYM"
      },
      "outputs": [],
      "source": [
        "x_train, x_valid, y_train, y_valid = train_test_split(train_data, train_targets, test_size=0.2, shuffle= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RZkrzDbufLP"
      },
      "outputs": [],
      "source": [
        "# Build the model. One hidden layer with 64 hidden units. Since it is a regression analysis the output shape will be 1 (linear)\n",
        "backend.clear_session()\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, activation = 'relu', input_shape = (train_data.shape[1],)))\n",
        "model.add(layers.Dense(64, activation = 'relu'))\n",
        "model.add(layers.Dense(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqurQ05UnkYY"
      },
      "outputs": [],
      "source": [
        "# Compile the model. MSE = mean squared error is a typical loss function for regression.  MAE = mean absolute error, a common regression metric\n",
        "model.compile(optimizer = 'adam', loss  = 'mse', metrics=['mae'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whBAFqW-nkYc"
      },
      "outputs": [],
      "source": [
        "# Fit the model to the data.  I set verbose = 0 so we will not see any output.\n",
        "history = model.fit(x_train,\n",
        "                   y_train,\n",
        "                   epochs = 1000,\n",
        "                   batch_size=16,\n",
        "                   validation_data=(x_valid, y_valid),\n",
        "                   verbose = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-x0mykvnkYg"
      },
      "outputs": [],
      "source": [
        "# Use this bit of code to view the History output.\n",
        "hist = pd.DataFrame(history.history)\n",
        "print(hist.tail())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bi1dcBlInkYp"
      },
      "outputs": [],
      "source": [
        "#Plot the loss and MAE vs epochs\n",
        "history_dict = history.history\n",
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "acc_values = history_dict['mae']\n",
        "val_acc_values = history_dict['val_mae']\n",
        "epochs = range(1, len(history_dict['mae']) + 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OaSQiWkgnkYt"
      },
      "outputs": [],
      "source": [
        "#Plot the loss epochs\n",
        "plt.plot(epochs, loss_values, 'r', label = 'Training loss')\n",
        "plt.plot(epochs, val_loss_values, 'b', label = 'Validation loss')\n",
        "plt.ylim(0,20)\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6ozqqvOnkY0"
      },
      "outputs": [],
      "source": [
        "#Plot MAE vs epochs\n",
        "plt.plot(epochs, acc_values, 'r', label = 'Training MAE')\n",
        "plt.plot(epochs, val_acc_values, 'b', label = 'Validation MAE')\n",
        "plt.ylim(0,5)\n",
        "plt.title('Training and validation MAE')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MAE')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuZzFD0kwqlc"
      },
      "outputs": [],
      "source": [
        "#Plot MAE vs epochs\n",
        "plt.plot(epochs, acc_values, 'r', label = 'Training MAE')\n",
        "plt.plot(epochs, val_acc_values, 'b', label = 'Validation MAE')\n",
        "plt.ylim(2,3)\n",
        "plt.xlim(50,200)\n",
        "plt.title('Training and validation MAE')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MAE')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SiaQGbFKnkY7"
      },
      "outputs": [],
      "source": [
        "# Apply the model to the test data and view the results.\n",
        "results = model.evaluate(test_data, test_targets)\n",
        "print(results)\n",
        "print(model.metrics_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtlwzqXXnkY_"
      },
      "outputs": [],
      "source": [
        "# It looks like validation loss and mean absolute error increase after about 200 epochs.\n",
        "# Lets remake the model with only 200 epochs\n",
        "\n",
        "backend.clear_session()\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, activation = 'relu', input_shape = (train_data.shape[1],)))\n",
        "model.add(layers.Dense(64, activation = 'relu'))\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.compile(optimizer = 'adam', loss  = 'mse', metrics=['mae'])\n",
        "\n",
        "history = model.fit(x_train,\n",
        "                   y_train,\n",
        "                   epochs = 200,\n",
        "                   batch_size=16,\n",
        "                   validation_data=(x_valid, y_valid),\n",
        "                   verbose = 0)\n",
        "\n",
        "results = model.evaluate(test_data, test_targets)\n",
        "print(results)\n",
        "print(model.metrics_names)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_Hx6DOYB3Hat"
      },
      "source": [
        "### We will incorporate K-Fold cross validation to help improve our model\n",
        "Since we have so few data points it makes sence to train and validate on all the data in k-fold batches. We will use `scikit-learn` to do the [k-fold cross validation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html ).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwzO697PnkYR"
      },
      "outputs": [],
      "source": [
        "# This time when we build the model we want to wrap it in a function because\n",
        "# we will be building a model for each k-fold of our validation steps.\n",
        "# To make sure we are comparing apples to apples lets use the same parameters\n",
        "# we used above.\n",
        "\n",
        "def build_model():\n",
        "  backend.clear_session()\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Dense(64, activation = 'relu', input_shape = (train_data.shape[1],)))\n",
        "  model.add(layers.Dense(64, activation = 'relu'))\n",
        "  model.add(layers.Dense(1))\n",
        "  model.compile(optimizer = 'adam', loss  = 'mse', metrics=['mae'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZixWlotOh-hr"
      },
      "outputs": [],
      "source": [
        "# Now we program in the KFold validation from sklearns\n",
        "# We will use 4 Kfold steps for validation on the original train data\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "k = 4 # Number of splits\n",
        "kf = KFold(n_splits = k, shuffle=True) # Call the KFold function\n",
        "all_val_mae_scores=[] # Where we will save all of the outputs \n",
        "num_epochs = 1000\n",
        "num_batch = 16\n",
        "# We have to run a loop and save all the mae scores from each of our 4 \n",
        "# KFold splits \n",
        "for k_train_index, k_val_index in kf.split(train_data, train_targets): \n",
        "  model = build_model()\n",
        "  history = model.fit(train_data[k_train_index], train_targets[k_train_index], \n",
        "                      validation_data = (train_data[k_val_index], train_targets[k_val_index]),\n",
        "                      epochs = num_epochs, batch_size=num_batch, verbose = 0)\n",
        "  all_val_mae_scores.append(history.history['val_mae'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0E2wUn0tFrx"
      },
      "outputs": [],
      "source": [
        "# Look at all the scores\n",
        "# all_val_mae_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPu7EoMbwZvW"
      },
      "outputs": [],
      "source": [
        "# We need to average each of the 4 KFold mae scores\n",
        "average_val_mae = [np.mean([x[i] for x in all_val_mae_scores]) for i in range(num_epochs)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IrzJurYgDV85"
      },
      "outputs": [],
      "source": [
        "# Plot the average Val \n",
        "plt.plot(range(1, len(average_val_mae) + 1), average_val_mae)\n",
        "plt.ylim()\n",
        "plt.xlim()\n",
        "plt.title('Average Validation MAE from K-Fold CV')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Average Val MAE')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZXaSZu_wy4J"
      },
      "outputs": [],
      "source": [
        "# Lets zoom in on that plot to find the optimal number of epochs to run.\n",
        "plt.plot(range(1, len(average_val_mae) + 1), average_val_mae)\n",
        "plt.ylim(2,3)\n",
        "plt.xlim(50,400)\n",
        "plt.title('Average Validation MAE from K-Fold CV')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Average Val MAE')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baJRUiq-xcc5"
      },
      "outputs": [],
      "source": [
        "# Rerun the model with the optimal epochs.\n",
        "k = 4\n",
        "kf = KFold(n_splits = k)\n",
        "all_mae_scores=[]\n",
        "num_epochs = 225\n",
        "num_batch = 16\n",
        "for k_train, k_val in kf.split(train_data, train_targets):\n",
        "  model = build_model()\n",
        "  history = model.fit(train_data[k_train], train_targets[k_train], \n",
        "                      validation_data = (train_data[k_val], train_targets[k_val]),\n",
        "                      epochs = num_epochs, batch_size=num_batch, verbose = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlKrAzuk3GNe"
      },
      "outputs": [],
      "source": [
        "# Evaluate our model on the test data.\n",
        "results = model.evaluate(test_data, test_targets)\n",
        "print(results)\n",
        "print(model.metrics_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWl3UV2q_qVo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Boston_Housing_Example.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
