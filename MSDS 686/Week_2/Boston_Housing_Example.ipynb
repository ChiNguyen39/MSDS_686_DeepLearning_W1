{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Boston_Housing_Example.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIQy8YMFnkXR"
      },
      "source": [
        "### This Example was adapted from Deep Learning with Python Chapter 4 Chollet, F. (2021). Deep Learning with Python (2nd ed.). Greenwich, CT, USA: Manning Publications Co."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_8G7ob6nkXW"
      },
      "source": [
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1aSfEnAnkXi"
      },
      "source": [
        "### This is an example of a neural net regression analysis problem using the prepackaged Boston Housing dateset from the Keras library.  The Boston Housing dataset has only 506 samples. We will use a deep neural net to predict Boston housing prices from 13 features.  You can read more about the dataset here: https://www.kaggle.com/c/boston-housing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the seed for reproducability\n",
        "import numpy as np\n",
        "np.random.seed(1)"
      ],
      "metadata": {
        "id": "FT3JmBMaE04T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RctHBZqrnkXk"
      },
      "source": [
        "# Import the boston housing data set\n",
        "from tensorflow.keras.datasets import boston_housing\n",
        "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NnkCFDMnkXq"
      },
      "source": [
        "# A total of 506 samples split between train and test data.  Each have 13 features used in predicting Boston House prices\n",
        "print(train_data.shape)\n",
        "print(test_data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4sJbebfnkXw"
      },
      "source": [
        "# The targets are Boston housing prices in thousands of dollars\n",
        "print(train_targets[1:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlHIZBWmnkX3"
      },
      "source": [
        "# The featurs have very differnt ranges and values\n",
        "print(train_data[0:5,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_UEFTXOnkX8"
      },
      "source": [
        "# Since all the features have very differnt ranges and values it is best to normalize the data.  We will subtract the mean value from each feature and divide by one standard deviation unit\n",
        "# This will center the data in each feature column around zero and has a unit standard deviation\n",
        "mean = train_data.mean(axis = 0)\n",
        "train_data -= mean\n",
        "\n",
        "std = train_data.std(axis = 0)\n",
        "train_data /= std\n",
        "\n",
        "test_data -= mean\n",
        "test_data /= std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtUTZXI4nkYB"
      },
      "source": [
        "# Now the featurs have very similar ranges and values\n",
        "print(train_data[0:5,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0ilNSbFnkYH"
      },
      "source": [
        "# Import libraries\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import backend\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44R9kzwFnkYM"
      },
      "source": [
        "x_train, x_valid, y_train, y_valid = train_test_split(train_data, train_targets, test_size=0.2, shuffle= True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model. One hidden layer with 64 hidden units. Since it is a regression analysis the output shape will be 1 (linear)\n",
        "backend.clear_session()\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, activation = 'relu', input_shape = (train_data.shape[1],)))\n",
        "model.add(layers.Dense(64, activation = 'relu'))\n",
        "model.add(layers.Dense(1))"
      ],
      "metadata": {
        "id": "3RZkrzDbufLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqurQ05UnkYY"
      },
      "source": [
        "# Compile the model. MSE = mean squared error is a typical loss function for regression.  MAE = mean absolute error, a common regression metric\n",
        "model.compile(optimizer = 'adam', loss  = 'mse', metrics=['mae'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whBAFqW-nkYc"
      },
      "source": [
        "# Fit the model to the data.  I set verbose = 0 so we will not see any output.\n",
        "history = model.fit(x_train,\n",
        "                   y_train,\n",
        "                   epochs = 1000,\n",
        "                   batch_size=16,\n",
        "                   validation_data=(x_valid, y_valid),\n",
        "                   verbose = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-x0mykvnkYg"
      },
      "source": [
        "# Use this bit of code to view the History output.\n",
        "hist = pd.DataFrame(history.history)\n",
        "print(hist.tail())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bi1dcBlInkYp"
      },
      "source": [
        "#Plot the loss and MAE vs epochs\n",
        "history_dict = history.history\n",
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "acc_values = history_dict['mae']\n",
        "val_acc_values = history_dict['val_mae']\n",
        "epochs = range(1, len(history_dict['mae']) + 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaSQiWkgnkYt"
      },
      "source": [
        "#Plot the loss epochs\n",
        "plt.plot(epochs, loss_values, 'r', label = 'Training loss')\n",
        "plt.plot(epochs, val_loss_values, 'b', label = 'Validation loss')\n",
        "plt.ylim(0,20)\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6ozqqvOnkY0"
      },
      "source": [
        "#Plot MAE vs epochs\n",
        "plt.plot(epochs, acc_values, 'r', label = 'Training MAE')\n",
        "plt.plot(epochs, val_acc_values, 'b', label = 'Validation MAE')\n",
        "plt.ylim(0,5)\n",
        "plt.title('Training and validation MAE')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MAE')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot MAE vs epochs\n",
        "plt.plot(epochs, acc_values, 'r', label = 'Training MAE')\n",
        "plt.plot(epochs, val_acc_values, 'b', label = 'Validation MAE')\n",
        "plt.ylim(2,3)\n",
        "plt.xlim(50,200)\n",
        "plt.title('Training and validation MAE')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MAE')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DuZzFD0kwqlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiaQGbFKnkY7"
      },
      "source": [
        "# Apply the model to the test data and view the results.\n",
        "results = model.evaluate(test_data, test_targets)\n",
        "print(results)\n",
        "print(model.metrics_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtlwzqXXnkY_"
      },
      "source": [
        "# It looks like validation loss and mean absolute error increase after about 200 epochs.\n",
        "# Lets remake the model with only 200 epochs\n",
        "\n",
        "backend.clear_session()\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, activation = 'relu', input_shape = (train_data.shape[1],)))\n",
        "model.add(layers.Dense(64, activation = 'relu'))\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.compile(optimizer = 'adam', loss  = 'mse', metrics=['mae'])\n",
        "\n",
        "history = model.fit(x_train,\n",
        "                   y_train,\n",
        "                   epochs = 120,\n",
        "                   batch_size=16,\n",
        "                   validation_data=(x_valid, y_valid),\n",
        "                   verbose = 0)\n",
        "\n",
        "results = model.evaluate(test_data, test_targets)\n",
        "print(results)\n",
        "print(model.metrics_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We will incorporate KFold cross validation to help improve our model.  Since we have so few data points it makes sence to train and validate on all the data in Kfold batchs. We will use scikit to do the KFold cross validation.  https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html \n"
      ],
      "metadata": {
        "id": "_Hx6DOYB3Hat"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwzO697PnkYR"
      },
      "source": [
        "# This time when we build the model we want to wrap it in a function because\n",
        "# because we will be building a model for each KFold of our validation steps.\n",
        "# To make sure we are comparing apples to apples lets use the same parameters\n",
        "# we used above.\n",
        "\n",
        "def build_model():\n",
        "  backend.clear_session()\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Dense(64, activation = 'relu', input_shape = (train_data.shape[1],)))\n",
        "  model.add(layers.Dense(64, activation = 'relu'))\n",
        "  model.add(layers.Dense(1))\n",
        "  model.compile(optimizer = 'adam', loss  = 'mse', metrics=['mae'])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we program in the KFold validation from sklearns\n",
        "# We will use 4 Kfold steps for validation on the original train data\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "k = 4 # Number of splits\n",
        "kf = KFold(n_splits = k, shuffle=True) # Call the KFold function\n",
        "all_val_mae_scores=[] # Where we will save all of the outputs \n",
        "num_epochs = 1000\n",
        "num_batch = 16\n",
        "# We have to run a loop and save all the mae scores from each of our 4 \n",
        "# KFold splits \n",
        "for k_train_index, k_val_index in kf.split(train_data, train_targets): \n",
        "  model = build_model()\n",
        "  history = model.fit(train_data[k_train_index], train_targets[k_train_index], \n",
        "                      validation_data = (train_data[k_val_index], train_targets[k_val_index]),\n",
        "                      epochs = num_epochs, batch_size=num_batch, verbose = 0)\n",
        "  all_val_mae_scores.append(history.history['val_mae'])"
      ],
      "metadata": {
        "id": "ZixWlotOh-hr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at all the scores\n",
        "# all_val_mae_scores"
      ],
      "metadata": {
        "id": "Y0E2wUn0tFrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to average each of the 4 KFold mae scores\n",
        "average_val_mae = [np.mean([x[i] for x in all_val_mae_scores]) for i in range(num_epochs)]"
      ],
      "metadata": {
        "id": "yPu7EoMbwZvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the average Val \n",
        "plt.plot(range(1, len(average_val_mae) + 1), average_val_mae)\n",
        "plt.ylim()\n",
        "plt.xlim()\n",
        "plt.title('Average Validation MAE from K-Fold CV')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Average Val MAE')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IrzJurYgDV85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets zoom in on that plot to find the optimal number of epochs to run.\n",
        "plt.plot(range(1, len(average_val_mae) + 1), average_val_mae)\n",
        "plt.ylim(2,3)\n",
        "plt.xlim(50,400)\n",
        "plt.title('Average Validation MAE from K-Fold CV')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Average Val MAE')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gZXaSZu_wy4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rerun the model with the optimal epochs.\n",
        "k = 4\n",
        "kf = KFold(n_splits = k)\n",
        "all_mae_scores=[]\n",
        "num_epochs = 225\n",
        "num_batch = 16\n",
        "for k_train, k_val in kf.split(train_data, train_targets):\n",
        "  model = build_model()\n",
        "  history = model.fit(train_data[k_train], train_targets[k_train], \n",
        "                      validation_data = (train_data[k_val], train_targets[k_val]),\n",
        "                      epochs = num_epochs, batch_size=num_batch, verbose = 0)"
      ],
      "metadata": {
        "id": "baJRUiq-xcc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate our model on the test data.\n",
        "results = model.evaluate(test_data, test_targets)\n",
        "print(results)\n",
        "print(model.metrics_names)"
      ],
      "metadata": {
        "id": "IlKrAzuk3GNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CWl3UV2q_qVo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}